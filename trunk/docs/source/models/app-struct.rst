
.. _app-structure:

Katmanja application structure
------------------------------

Katmanja applications should be modularized into a pipeline
of :ref:`messages<messages-term>`,
:ref:`containers<container-term>`, and :ref:`models<model-term>`.
Even very complex applications are implemented
as a series of relatively simple messages, containers, and models.

- An input message describes the structure of the data coming into a model.

  - The message in the pipeline ingests your data
    from an external data source.  See :ref:`ingest-data-model`
    and is then fed as an input message to the first model.
  - The first model executes a small number of related calculations
    on that data, usually to prepare and pre-process the ingested data

- The model outputs its results into an output message,
  which may then be the input message to the next model in the pipeline.
- Containers hold reference data that the model uses
  to process the message data flowing through the pipeline.
  For example, one might need a database of information
  about employees or customer preferences.
  A container is defined in a JSON file with a structure
  that is similar to that used for messages.
  It is represented in Kamanja as a class,
  generated by the message compiler through
  the :ref:`metadata API<metadataapi-term>` interface.
- Any message or container that has the "persist" property
  is stored in the cluster's data warehouse.
- A model configuration file specifies the execution order
  for the models in the application pipeline.
  Kamanja uses :ref:`DAG<dag-term>` to implement this.
  Each message executes all models in the pipeline sequentially;
  multiple messages can execute the same pipeline in parallel.
- One model can consume multiple input messages
  and can correlate data between different data streams.
  To implement this, define one key that is identical
  for each message definition that needs to be correlated.
  The model can then use this key to look up and correlate data
  between the different messages that represent the data stream.
  You also need to ensure that corresponding data in different data streams
  has the same format and definition;
  this should be done in the pre-processing models
  before the data is fed to the analytical models.
- To dynamically add a new field to an existing message,
  you must modify the :ref:`message definition<message-def-config-ref>`
  and resubmit the message definition to the metadata.
  Messages processed after the message definition is updated
  use the new definition.

:ref:`Adapter message bindings<message-bindings-term>`
control the flow of messages and models.  For example:

- The output message binding defines how messages are passed
  from model to model; see :ref:`adapters-output-guide`.
- The output message definition is written to the metadata API;
  see http://kamanja.org/wiki/output-messages/
- Results can be persisted to HDFS, Cassandra, ElasticSearch,
  or to a local file system;
  see :ref:`smart-file-adapter`.

Implementing a Kamanja application
----------------------------------

An application that runs on Kamanja contains three major components:

- models, messages, and containers to analyze your data.
- models and messages to injest and preprocess your data.
- messages to output your data to storage
  or real-time analytics tools of your choice.

Each of these is discussed in the following sections.

Code to analyze your data
~~~~~~~~~~~~~~~~~~~~~~~~~

The code to analyze your data is written as a series of input messages,
models, output messages, and adapters.
The basic steps to implement this are:

- Write message definitions for the data
  that will flow into and out of each model in the application
  and container definitions for any data that is referenced by the application.
- Add the message and container definitions to Kamanja.
- Write a model for each calculation required in the application.
- Create model compilation configuration files.
- Add the model compilation configuration and the models to Kamanja.
- Write adapter message bindings and add them to Kamanja.
- Create data and present it to Kamanja;
  see Data Ingestion and Preprocessing below.
- Push the data to Kamanja.
- Kamanja executes the models and produces output.

:ref:`intro-simple-scala-tut` is a tutorial that introduces you
to the process of integrating a simple application into Kamanja.

Model exception handling
~~~~~~~~~~~~~~~~~~~~~~~~

Proper exception handling for a model is extremely important,
especially in the pre-processing models
that first receive new data and need to ensure that the data is valid.
Any invalid data that is detected can be rejected or perhaps repaired;
it is up to the model writer to determine the appropriate action.
It is critical is to ensure that invalid data is never passed
as an output message to another model.
While executing the exception handling code does have
some small impact on the performance of the model,
this is much less serious than the havoc that can ensue
if you pass invalid data down the pipeline.

For information about how Kamanja processes exceptions,
see :ref:`failure-tracking-admin`.


