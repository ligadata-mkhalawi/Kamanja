
.. _model-validation-arch:

Model Validation
================

The model types listed below were taken from the following diagram
called “Kamanja Integration with Software and Algorithms”
(updated 5/20/2016):

.. image:: /_images/MatrixR.png

The following model types from PMML producers (vendors) were validated:

- SAS (vendor), using PMML generated by their Enterprise Miner software:

  - Decision Trees (CART, C5.0, Cubist)
  - Neural Net
  - Regression (linear, logistic)

- R (vendor), using many packages including Rattle:

  - Decision Trees (CART, C5.0, Cubist)
  - Decision Trees (Random Forest)
  - Naive Bayes
  - Neural Net
  - Regression (linear, logistic)
  - Support Vector Machines (SVM) [Note: There is currently a reported bug
    in the R PMML package, stating SVM PMML models cannot currently
    be generated.]
  - Cluster (i.e. K-Means)

Data Sets Used for Validating Kamanja PMML Execution of Algorithms
------------------------------------------------------------------

The data set used, HMEQ, involves home mortgages,
with the target indicating which mortgages went bad
after the loan was taken out.
For more details about the HMEQ data set, read HMEQ-SAS-doc.pdf.
This doc was reproduced from the SAS site.
The input fields have many missing values,
which are handled differently by different algorithms or vendors.

.. See `here<http://kamanja.org/wiki/score-difference-report-tool-2/>`_
.. for the tool to generate the score difference report.

SAS Models
----------

For each algorithm:

#. SAS, the vendor, provided LigaData with a) models,
   b) the scored data and, c) the generated PMML for regression,
   decision tree, and neural net algorithms.
   When SAS provided this data, there was no prior preprocessing,
   such as handling missing data.
   The PMML did not perform any missing value preprocessing,
   so, by default, missing scores are expected
   if any input field has a missing value.
#. The PMML was loaded into Kamanja and Kamanja ran the provided PMML,
   scoring the same data, but producing the additional
   Kamanja execution generated score.
#. For each record, the Kamanja-generated score was subtracted
   from the original SAS-generated score.
   The expected difference between scores is 0 or less than 0.00001
   (some small value).

R Models
--------

R, using the Rattle package, was used to preprocess and clean up the data.
In this release, the preprocessing is not part of the testing,
but the updated data version was used for testing.
The preprocessing steps included:

Preprocessing of Continuous Missing Values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Issue: By default, for most algorithms,
if any input value is missing, the produced score is missing.
One reasonable way to handle this is mean substitution of missing values.

Preprocessing of Categorical Missing Values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Issue: For categorical fields, missing values can be handled in two ways.
If there is a dominant category that is much more frequent than others,
then substitute with the most frequent category.
If there are many (that is, 5+ categories),
make the missing field a new category.

The reason field has values of HomeImp for living in the home
or for home improvement. This was much more frequent.
The other category is DebtCon for debt consolidation.
Missing values in the reason field were replaced with HomeImp.

The job field has six categorical values.
The number of missing records was more frequent
than two of the least frequent valid categories,
so it was decided to create a seventh categorical field value called missing.

Preprocessing of Categorical Values
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For categorical values that are “nominal” or do not have an order,
if the number of categories is less than 10 or 20,
it is common to create a “dummy encoding” or a “one-of-N” encoding.
The original categorical values may have been numeric, character, or string.
Many times, the new fields are named such as FieldName_value,
with 0/1 values where 1 is implied as TRUE.
For example, using the Job field as input,
create the following one-of-N encoding fields.


.. list-table::
   :widths: 15 15 15 15 15 25
   :header-rows: 1

   * - Job
     - Job Mgr
     - Job Missing
     - Job Other
     - Job ProfExe
     - Job Sales
   * - Other
     - 0
     - 0
     - 1
     - 0
     - 0
   * - Sales
     - 0
     - 0
     - 0
     - 0
     - 1
   * -
     - 0
     - 1
     - 0
     - 0
     - 0
   * - Mgr
     - 1
     - 0
     - 0
     - 0
     - 0


Preprocessing to Recode to a [0..1] Range
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Many algorithms, such as regression, neural network, SVM, or Kmeans clustering,
expect the input field ranges to be either a similar magnitude,
or in a [-1 .. +1] range, or a [0..1] range.
In order to preprocess once, consistently, for all algorithms,
it helps to perform this scaling. This would only affect continuous fields.

Preprocessing to Even Out Distributions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

(This type of preprocessing was not performed on the HMEQ data,
although it would have applied to the following fields:
value, derog, and delinq).

Note: In some cases, for fields with a long tail (or skewed) distribution
(such as money, revenue, number of transactions)
where there is a frequent occurrence of small values,
and a few, very large values, another type of preprocessing helps.
If 80% of the record values are in 10% of the range of a field,
preprocessing can help many data mining algorithms
by spreading out the distribution more broadly.

::

  If (3 < skew(field)) then safelog(field)

  Safelog(field){ #pseudo code to define this function
       If (field < 0) then
            Field_sign = -1 #the output value should have the same sign as the input value
       Else
            Field_sign = 1

                 # avoid log(0) because it is not valid
                 # avoid log(0..1) because the sign changes
       Return_field = log( 1 + abs(field)) * field_sign
       }

There were two stages of validation.
The first stage involved introducing the data set as raw data
to each of the algorithms.
The second stage involved introducing the data set
plus the score from ensemble modeling to each of the algorithms.
Ensemble modeling is running two or more models
and combining the results into a single score.
Choosing the right blend of models took the most time.

Updated Data (new in v1.6)
~~~~~~~~~~~~~~~~~~~~~~~~~~

The above preprocessing was used to create a new input data set,
called hmeq_prep3.csv, used for the following models.

There were updated R models for testing. See the model notebook.

#. First, the preprocessing was performed.
   Preprocessing with the hmeq_prep3.R and hmeq_prep3.
   Rattle files was performed,
   to create the hmeq_prep3.csv data file to be loaded into the models.
   This preprocessing was especially needed
   for the ensemble modeling (DAG) test.

   - For the job categorical field, a binary encoding was created.
     The missing values were included.
   - For the mortgage reason code, a binary encoding was created.
   - For the continuous input fields with missing values,
     the missing fields were replaced with the average value.
   - For the continuous input fields,
     the fields were rescaled to [0..1],
     for the neural net and kmeans algorithms.

#. A number of models over many different algorithms were created.
   See the model notebook.
#. For the DAG or ensemble models,
   the best of the five algorithms
   that could generate a PMML model (not SVM),
   were chosen and a new dataset, hmeq5scores.csv, was created.
   This is the output of the first stage of the DAG,
   and the input to the second stage.
#. A series of second stage models to compete for the second stage,
   starting with file names hmeq5scores_R*, were created.

Relevant Information
--------------------

    Vendor: SAS / Software: Enterprise Minor
    Vendor: R / Software: Rattle

Algorithm: Decision Trees
~~~~~~~~~~~~~~~~~~~~~~~~~

- Data Set: hmeq.csv
- Source Model: hmeq_SAS_DecisionTree.sas
- Execution Log Text File: N/A
- PMML: hmeq_SAS_DecisionTree_model.xml
- Output results: hmeq_SAS_DecisionTree_score_all.csv

Algorithm: Neural Net
~~~~~~~~~~~~~~~~~~~~~

- Data Set: hmeq.csv
- Source Model : hmeq_SAS_neuralnet.sas
- Execution Log Text File: N/A
- PMML: hmeq_SAS_DecisionTree_model.xml
- Output results: hmeq_SAS_neuralnet_score_all.csv

Algorithm: Regression
~~~~~~~~~~~~~~~~~~~~~

- Data Set: hmeq.csv
- Source Model : hmeq_SAS_regrLogistic.sas
- Execution Log Text File: N/A
- PMML: hmeq_SAS_regrLogistic_model.xml
- Output results: hmeq_SAS_regrLogistic_score_all.csv



